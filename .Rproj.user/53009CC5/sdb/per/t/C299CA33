{
    "collab_server" : "",
    "contents" : "---\ntitle: \"PCA of ROIs from Sex*Deprivation Analysis\"\nauthor: \"Lucy King\"\noutput: html_notebook\n---\n##Load data\n```{r load_data}\nlibrary(psych)\nlibrary(factoextra)\nlibrary(corrplot)\nlibrary(gpairs)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(RColorBrewer)\nlibrary(directlabels)\nlibrary(glmpath)\n\ninc_46all <- read.csv(\"~/Desktop/ELS/dep_thrt_TBM/data/Inc_Interac_46ROIs_ALL.csv\")\n```\n\n##Sum bilateral correlated clusters or correlated clusters in the same region\n```{r composites}\ninc_46all$Bi_Hipp <- inc_46all$Neg_37 + inc_46all$Neg_39\ninc_46all$PosteriorThalamicRad <- inc_46all$Neg_35 + inc_46all$Neg_24\ninc_46all$Thalamus <- inc_46all$Neg_23 + inc_46all$Neg_34\ninc_46all$IFG <- inc_46all$Neg_16 + inc_46all$Neg_27\n``` \n\n\n```{r correlations}\ncor_incpca <- inc_46all[,2:47]\ncor_incpca <- cor(cor_incpca, use = \"pairwise.complete.obs\")\ncor_incpca <- corrplot(cor_incpca, order = \"hclust\", tl.col='black', tl.cex=.75)\nwrite.csv(cor_incpca, \"~/Desktop/ELS/dep_thrt_TBM/depthrt_sync/results/corr_inc_46all.csv\")\n```\n\n```{r pca}\n#drop ID column\ninc_pca_nID <- inc_46all[2:47]\n\ninc_pca1 <- prcomp(inc_pca_nID, scale = T) #by default prcomp centers the variable mean = 0, scale = T also standardizing them sd = 1\nfviz_eig(inc_pca1) #scree plot\nsummary(inc_pca1) #includes variance explained by each\nget_eigenvalue(inc_pca1) #eigen values\n\n(inc_pca1$rotation) #rotations are loadings = correlation of the component with the variable\n(inc_pca1$rotation)^2 #squared loadings; the sum of the squared coefficients of correlation between a variable and all the components is equal to 1; thus, interpreted as the proportion of variance in the variable explaned by the component\n\nfviz_pca_var(inc_pca1,\n             col.var = \"contrib\", # Color by contributions to the PC\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     # Avoid text overlapping\n             )\n#The closer a variable is to the circle of correlations, the better we can reconstruct this variable from the first two components (and the more important it is to interpret these components); the closer to the center of the plot a variable is, the less important it is for the first two components. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.\n```\n\n\n##Regularized regression\nWe have observations on n units of a DV and p predictors, where p >> n.  (In typical regressions, n ≈ 10*p > p.)  In principle, using any n of the p predictors allows us to ‘predict’ the n DV values perfectly.  However, prediction of new observations is likely to be ‘poor’.  This problem is ill-conditioned.  The data set (or system) is said to be over-determined.\n\nRegularization resolves non-complementary goals of wants to drive down the sum of squared errors in the model for the observed data but also wanting your model to predict new data (avoding overfitting).\n\nThe elastic net (Zou & Hastie, 2005) was designed to achieve the sparseness of the LASSO, and the ability of ridge regression to select correlated features.  The penalty used is a weighted average of penalties on the L1-norm and the L2-norm. This model can be rewritten as a type of LASSO and, therefore, can be estimated with the efficiency of the LASSO. The default in glmpath() is an elastic net with a very small penalty on the L2-norm -- basically you are getting sparseness without losing important information, which is particularly relevent for brain data. It is like a stretchable fishing net that retains ‘all the big fish’.\n\nChoosing the value of λ under the L1-norm, which results in some bi  = 0, is like choosing which predictors to include in a stepwise regression.The optimal λ (penalty on complexity) is the value at which the AIC (or BIC or CV error) of the optimal model is a minimum. For each λ in (0, λ*), glmpath() provides the optimal model. How to determine the optimal λ? Choosing the value of λ under the L1-norm (which optimizes sparse solutions), which results in some bi  = 0, is like choosing which predictors to include in a stepwise regression.\n\n\n```{r glmpath_all}\ninc_gp <- merge(inc_46all, roi_inc_symp_comp[, c(\"ELS_ID\", \"Age\", \"Tanner\", \"White\", \"ICV\", \"Male\", \"T1_CBCL_old_Externalizing_Problems_Total_Score\")], by = \"ELS_ID\")\n\n#remove individal variables for variables that were summed\ninc_gp <- within(inc_gp, rm(Neg_37, Neg_39, Neg_35, Neg_24, Neg_23, Neg_34, Neg_16, Neg_27))\n\n# for regularized regression...\n# make a MATRIX (not a dataframe!) of our predictors\npredictor = as.matrix(inc_gp[, 2:47])\n\n# make a VECTOR (not a variable in a dataframe!) of our outcome \nexternalizing = c(inc_gp$T1_CBCL_old_Externalizing_Problems_Total_Score)\n\nregR_inc = glmpath(x = predictor, y = externalizing, family = gaussian)\nregR_inc\n```\n\n```{r glmpath_boys}\ninc_gp_boys <- subset(inc_gp, Male == 1)\n\npredictorB = as.matrix(inc_gp_boys[, 2:51])\npredictorB <- scale(predictorB)\nexternalizingB = scale(c(inc_gp_boys$T1_CBCL_old_Externalizing_Problems_Total_Score))\n\nregR_incB = glmpath(x = predictorB, y = externalizingB, family = gaussian)\nregR_incB\nx <- as.data.frame(summary(regR_incB))\n\nsummary(regR_incB)[summary(regR_incB)$AIC == min(summary(regR_incB)$AIC),] #AIC is minimal at step 29\nsummary(regR_incB)[summary(regR_incB)$BIC == min(summary(regR_incB)$BIC),] #BIC is minimal at step 7\n#find lambda that minimizes BIC\nlambda_minBIC <- d_predictor %>%\n  filter(bic == min(bic)) %>%\n  select(lambda, bic) %>%\n  unique()\nlambda_minBIC\nregR_incB$b.predictor\n\nboot <- bootstrap.path(predictorB, externalizingB, B = 100)\n\nd_predictor <- fit$b.predictor %>% # extract the coefficient estimates\n  data.frame() %>% # turn them into a dataframe\n  add_rownames(var = \"step\") %>% # add the rownames as a \"step\" variable\n  mutate(lambda = fit$lambda, # add the lambda values for each step\n         aic = fit$aic, # add AIC for each step\n         bic = fit$bic) %>% # add BIC for each step\n  gather(brains, coeff, Pos_28:ICV) %>% # turn into longform\n  arrange(step, brains) # arrange by step number and selfrating\n\nggplot(data = d_predictor, aes(x = lambda, y = coeff, color = brains)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", alpha = 0) +\n  theme_bw() +\n  theme(text = element_text(size = 20),\n        legend.position = \"none\") + \n  geom_dl(aes(label = brains), method = list(\"first.qp\", cex = 1)) +\n  xlim(-5, max(d_predictor$lambda))\n\nlambda_minBIC <- d_predictor %>%\n  filter(bic == min(bic)) %>%\n  select(lambda, bic) %>%\n  unique()\nlambda_minBIC\n\nggplot(data = d_predictor, aes(x = lambda, y = bic)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", alpha = 0) +\n  theme_bw() +\n  theme(text = element_text(size = 20),\n        legend.position = \"none\") +\n  geom_vline(aes(xintercept = lambda_minBIC$lambda), lty = 3, color = \"red\") +\n  annotate(\"text\", x = lambda_minBIC$lambda + 1, y = lambda_minBIC$bic,\n           label = paste0(\"lambda = \", round(lambda_minBIC$lambda, 2)),\n           hjust = 0, size = 5, color = \"red\")\n\nregR_incB$b.predictor[7,] # \"7\" corresponds to the best step, above\n\ninc_gp_boys_nID$T1_CBCL_old_Externalizing_Problems_Total_Score\nres11 <- lm(T1_CBCL_old_Externalizing_Problems_Total_Score ~.,inc_gp_boys_nID)\nres12 <- step(res11, direction = \"both\")\nsummary(res12)\n\n#as lambda decreases, df(n of non-zero coeffs) increase; %Dev is like R2\nfit <- glmnet(predictorB, externalizingB, family = \"gaussian\")\nfit\ncvfit <- cv.glmnet(predictorB, externalizingB, family = \"gaussian\")\ncvfit$lambda.1se\ncvfit$lambda.min\nplot(cvfit)\nplot(fit, xvar = \"lambda\")\nplot(fit, xvar = \"dev\")\nany(fit$lambda == 0.28)\ncoef.exact = coef(fit, x = predictorB, y = externalizingB, s = 0.28, exact = FALSE)\ncoef.exact\n\nx <- lm(T1_CBCL_old_Externalizing_Problems_Total_Score ~ Pos_28 + Pos_8 + Neg_38 + Neg_35 + Neg_31 + Neg_26 + Neg_18 , data =inc_gp_boys)\nx <- lm(T1_CBCL_old_Externalizing_Problems_Total_Score ~ Neg_38 + Neg_18, data =inc_gp_boys)\nsummary(x)\n\nx <- lm(T1_CBCL_old_Externalizing_Problems_Total_Score ~ Neg_35, data =inc_gp_boys)\nsummary(x)\nvif(x)\n```\nfirst line closest is mininum, second line is more conservative -- within one SD of the minimum; better to use more conservative if you want parsimony\n\n\n```{r glmpath_girls}\ninc_gp_girls <- subset(inc_gp, Male == 0)\n\npredictorG = as.matrix(inc_gp_girls[, 2:46])\n\nexternalizingG = c(inc_gp_girls$T1_CBCL_old_Externalizing_Problems_Total_Score)\n\nregR_incG = glmpath(x = predictorG, y = externalizingG, family = gaussian)\nregR_incG\nx <- as.data.frame(summary(regR_incB))\n\nsummary(regR_incB)[summary(regR_incG)$AIC == min(summary(regR_incG)$AIC),] #AIC is minimal at step 29\nsummary(regR_incB)[summary(regR_incG)$BIC == min(summary(regR_incG)$BIC),] #BIC is minimal at step 7\n#find lambda that minimizes BIC\nlambda_minBIC <- d_predictor %>%\n  filter(bic == min(bic)) %>%\n  select(lambda, bic) %>%\n  unique()\nlambda_minBIC\nregR_incB$b.predictor\n\nboot <- bootstrap.path(predictorB, externalizingB, B = 100)\n\nd_predictor <- regR_incB$b.predictor %>% # extract the coefficient estimates\n  data.frame() %>% # turn them into a dataframe\n  add_rownames(var = \"step\") %>% # add the rownames as a \"step\" variable\n  mutate(lambda = regR_incB$lambda, # add the lambda values for each step\n         aic = regR_incB$aic, # add AIC for each step\n         bic = regR_incB$bic) %>% # add BIC for each step\n  gather(brains, coeff, Pos_28:ICV) %>% # turn into longform\n  arrange(step, brains) # arrange by step number and selfrating\n\nggplot(data = d_predictor, aes(x = lambda, y = coeff, color = brains)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", alpha = 0) +\n  theme_bw() +\n  theme(text = element_text(size = 20),\n        legend.position = \"none\") + \n  geom_dl(aes(label = brains), method = list(\"first.qp\", cex = 1)) +\n  xlim(-5, max(d_predictor$lambda))\n\nlambda_minBIC <- d_predictor %>%\n  filter(bic == min(bic)) %>%\n  select(lambda, bic) %>%\n  unique()\nlambda_minBIC\n\nggplot(data = d_predictor, aes(x = lambda, y = bic)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", alpha = 0) +\n  theme_bw() +\n  theme(text = element_text(size = 20),\n        legend.position = \"none\") +\n  geom_vline(aes(xintercept = lambda_minBIC$lambda), lty = 3, color = \"red\") +\n  annotate(\"text\", x = lambda_minBIC$lambda + 1, y = lambda_minBIC$bic,\n           label = paste0(\"lambda = \", round(lambda_minBIC$lambda, 2)),\n           hjust = 0, size = 5, color = \"red\")\n\nregR_incB$b.predictor[7,] # \"7\" corresponds to the best step, above\n\n```\n\n\nlambda max is smallest value of lambda is the smallest value of lamda at which all coefficients are nonzero\nlambda min, close to 0, close to unrestricted fit\nas you move from lambda max to lambda min, coefficients become non zero (pop up)",
    "created" : 1522090171350.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2696807233",
    "id" : "C299CA33",
    "lastKnownWriteTime" : 1522437841,
    "last_content_update" : 1522437841236,
    "path" : "~/Desktop/ELS/dep_thrt_TBM/depthrt_sync/depthrt_scripts/PCA_sex_depr.Rmd",
    "project_path" : "PCA_sex_depr.Rmd",
    "properties" : {
        "chunk_output_type" : "inline",
        "tempName" : "Untitled1"
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}