---
title: "glmnet sex*income-to-needs analysis"
author: "Lucy King"
output: html_notebook
---

##Load data
```{r load_data}
#Libraries
library(ggplot2)
library(tidyverse)
library(readxl)
library(glmnet)
library(haven)

z_score <- function(x) {
  diff_mu <- x - mean(x, na.rm = T)
  sd <- sd(x, na.rm = T)
  diff_mu / sd
}

#Parameters

#datasets____________________________________________________________________
#extracted estimates for all 46 ROIs identified in sex*income model
inc_46all <- "~/Desktop/ELS/income_TBM/data/Inc_Interac_46ROIs_ALL.xlsx"

#covariates used in income TBM model
inc_TBM_cov <- "~/Desktop/ELS/income_TBM/data/TBM_cov_income.csv"

#symptoms
T1_CBCL <- "~/Desktop/ELS/income_TBM/data/ELS_T1S1_CBCL_old_1991_Final Profile Data_ID 1-218.sav"

```

##Join data
```{r}
covariates <- 
  read_csv(inc_TBM_cov) %>%
  mutate(ELS_ID = as.integer(ELS_ID)) 

rois <- 
  read_excel(inc_46all) %>%
  mutate(ELS_ID = as.integer(ELS_ID))

cbcl_externalizing <- 
  read_sav(T1_CBCL, user_na = 999) %>%
  mutate(ELS_ID = as.integer(ELS_ID)) %>%
  select(
    ELS_ID,
    externalizing = T1_CBCL_old_Externalizing_Problems_Total_Score
  )

inc_rr_boys <-
  covariates %>%
  filter(Male == 1) %>%
  left_join(rois, by = "ELS_ID") %>%
  left_join(cbcl_externalizing, by = "ELS_ID") %>%
  select(-Male, -ELS_ID)
```
##Regularized regression
We have observations on n units of a DV and p predictors, where p >> n.  (In typical regressions, n ≈ 10*p > p.)  In principle, using any n of the p predictors allows us to ‘predict’ the n DV values perfectly.  However, prediction of new observations is likely to be ‘poor’.  This problem is ill-conditioned.  The data set (or system) is said to be over-determined.

Regularization resolves non-complementary goals of wants to drive down the sum of squared errors in the model for the observed data but also wanting your model to predict new data (avoding overfitting).

The elastic net (Zou & Hastie, 2005) was designed to achieve the sparseness of the LASSO, and the ability of ridge regression to select correlated features. The penalty used is a weighted average of penalties on the L1-norm and the L2-norm. This model can be rewritten as a type of LASSO and, therefore, can be estimated with the efficiency of the LASSO. Basically you are getting sparseness without losing important information, which is particularly relevent for brain data. It is like a stretchable fishing net that retains ‘all the big fish’.

Choosing the value of λ under the L1-norm, which results in some bi  = 0, is like choosing which predictors to include in a stepwise regression.The optimal λ (penalty on complexity) is the value at which the AIC (or BIC or CV error) of the optimal model is a minimum. 

##Prep  data for `glmnet()` models in boys
All data are standardized so that coefficients can be interpreted as standardized. I am using the z-score function for externalizing because  `scale()` does not work on a vector. 

```{r glmnet_prep}
predictors = scale(inc_rr_boys[1:50], scale = TRUE)

externalizing = z_score(inc_rr_boys$externalizing)
```

##Fit glmnet model for income ROIs in boys
As lambda decreases, df(n of non-zero coeffs) increase; %Dev is like R2. In plot of lambda and deviance ratio, predictors are identified by their sequence numbers + 1 (e.g., the strongest predictor = 46 = 47). The +1 is due to the fact that the first "step" in the model is labeled "step 0". This is a confusing feature of the plot. 

```{r glmnet_inc}
fit_inc_boys <- glmnet(predictors, externalizing, alpha = .5, family = "gaussian")

plot(fit_inc_boys, xvar = "lambda", label = TRUE)
plot(fit_inc_boys, xvar = "dev", label = TRUE)
```

```{r}
print(fit_inc_boys)
```



##Fit cross-validation model for income ROIs in boys
Note also that the results of cv.glmnet are random, since the folds are selected at random. Users can reduce this randomness by running cv.glmnet many times, and averaging the error curves.
```{r inc_cv}
inc_lambdaMins <- c()
inc_lambda1SEs <- c()

for (i in 1:1000) {
  fit_inc_boys_cv <- cv.glmnet(predictors, externalizing, alpha = .5, family = "gaussian")
  inc_lambdaMins <- cbind(inc_lambdaMins, fit_inc_boys_cv$lambda.min)
  inc_lambda1SEs <- cbind(inc_lambda1SEs, fit_inc_boys_cv$lambda.1se)
}

```

```{r}
plot.cv.glmnet(fit_inc_boys_cv)
```

##Gather lambda values from CV for income ROIs in  boys
```{r}
inc_lambdaMins <- as.tibble(inc_lambdaMins)
inc_lambdaMins <-
  inc_lambdaMins %>%
  gather(run, lambdaMin, V1:V1000) 

inc_lambda1SEs <- as.tibble(inc_lambda1SEs)
inc_lambda1SEs <-
  inc_lambda1SEs %>%
  gather(run, lambda1SE, V1:V1000) 

inc_lambdas <- merge(inc_lambdaMins, inc_lambda1SEs, by = "run")
```

##Summarise results of cross-validation for income ROIs in boys
```{r}
inc_lambdas %>%
  ggplot(mapping = aes(x = "", y = lambdaMin)) +
  geom_boxplot() +
  coord_flip() +
  labs(
    x = ""
  )

inc_lambdas %>%
  ggplot(mapping = aes(x = "", y = lambda1SE)) +
  geom_boxplot() +
  coord_flip() +
  labs(
    x = ""
  )

inc_lambdas_summary <-
  inc_lambdas %>%
  summarise(
    min_mean = mean(lambdaMin),
    min_median = median(lambdaMin),
    min_sd = sd(lambdaMin),
    SE_mean = mean(lambda1SE),
    SE_median = median(lambda1SE),
    SE_sd = sd(lambda1SE)
  )
inc_lambdas_summary

range(inc_lambdas$lambdaMin)
range(inc_lambdas$lambda1SE)
```

We will use lambda + 1SE in order to select the most parsimonious model. This more conservative approach is best considering our small sample size. The optimal model is achieved at lambda1SE = .511. Across the 1000 runs, almost all lambda values fell between ~.55 and ~.62. Given the outlier, we will use the median lambda value across the runs.


##Print coeffs and plots for final model for income ROIs in boys with identified lambda
```{r}
coef(fit_inc_boys, s = inc_lambdas_summary$min_median)
coef(fit_inc_boys, s = inc_lambdas_summary$SE_median)

##final model 
coef(fit_inc_boys, s = 0.5107218)
```

**final coefficients**
1
(Intercept) -2.045581e-17
Age          .           
Tanner       .           
ICV          .           
White        .           
Pos_28      -1.704006e-02
Pos_27       .           
Pos_26       .           
Pos_25       .           
Pos_24       .           
Pos_23       .           
Pos_22       .           
Pos_21       .           
Pos_20       .           
Pos_19       .           
Pos_18       .           
Pos_17       .           
Pos_16       .           
Pos_15       .           
Pos_14       .           
Pos_13       .           
Pos_12       .           
Pos_11       .           
Pos_10       .           
Pos_9        .           
Pos_8       -6.261820e-02
Neg_39       .           
Neg_38       5.433491e-02
Neg_37       .           
Neg_36       .           
Neg_35       8.544712e-03
Neg_34       .           
Neg_33       .           
Neg_32       .           
Neg_31       5.353314e-02
Neg_30       .           
Neg_29       .           
Neg_28       .           
Neg_27       .           
Neg_26       4.091964e-02
Neg_25       .           
Neg_24       .           
Neg_23       .           
Neg_22       .           
Neg_21       .           
Neg_20       .           
Neg_19       .           
Neg_18       1.139653e-01
Neg_17       .           
Neg_16       .           
Neg_15       .           


##Test different alphas
```{r}
foldid = sample(1:10, size = length(externalizing), replace = TRUE)

cv1 = cv.glmnet(predictors, externalizing, foldid = foldid, alpha = 1)

cv.5 = cv.glmnet(predictors, externalizing, foldid = foldid, alpha = .5)

cv0 = cv.glmnet(predictors, externalizing, foldid = foldid, alpha = 0)

coef(cv.5, s = cv.5$lambda.1se)

par(mfrow = c(2,2))
plot(cv1)
plot(cv.5)
plot(cv0)

##plot alphas
plot(
  log(cv1$lambda), 
  cv1$cvm, 
  pch = 19, col = "red", xlab = "log(Lambda)", ylab = cv1$name
)
points(log(cv.5$lambda), cv.5$cvm, pch = 19, col = "grey")

points(log(cv0$lambda), cv0$cvm, pch = 19,col = "blue")

legend("topleft", legend = c("alpha = 1","alpha = .5","alpha 0"), pch = 19,col = c("red","grey","blue"))
```
